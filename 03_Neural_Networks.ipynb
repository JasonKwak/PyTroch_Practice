{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_Neural_Networks.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"d_eMjQh0R7Yh","colab_type":"text"},"source":["**Neural networks**는 **torch.nn** 패키지를 이용해서 만들 수 있다. \n","\n","이미 **autograd**에 대해서 살펴봤다. **nn**은 모델을 정의하고 차별화하기 위해서 **autograd**에 의존한다. **nn.Module**은 layer들과 출력을 반환하는 **forward(input)**을 포함하고 있다. "]},{"cell_type":"markdown","metadata":{"id":"V4yA-xtBS8Za","colab_type":"text"},"source":["일반적인 neural network의 학습 절차는 다음과 같다. \n","\n","1. 학습 가능한 매개변수(또는 가중치)가 있는 신경망을 정의\n","2. 입력 데이터셋에 따라서 반복\n","3. network를 통해서 input을 처리하고\n","4. 손실을 계산\n","5. gradient를 네트워크의 변수들로 다시 전파\n","6. 네트워크의 가중치를 업데이트:  **weight = weight - learning_rate*gradient**"]},{"cell_type":"markdown","metadata":{"id":"Xk_MdmV-V42F","colab_type":"text"},"source":["# Define the network"]},{"cell_type":"code","metadata":{"id":"6iWYog2rRJBg","colab_type":"code","colab":{}},"source":["import torch \n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpxYUx-dWCVA","colab_type":"code","outputId":"d25e919c-de4f-409d-901a-1286a17f1ff5","executionInfo":{"status":"ok","timestamp":1560842180974,"user_tz":-540,"elapsed":935,"user":{"displayName":"Sangjoon Kwak","photoUrl":"","userId":"14338642773681962512"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["class Net(nn.Module):\n","  \n","  def __init__(self):\n","    super(Net, self).__init__()\n","    \n","    self.conv1 = nn.Conv2d(1, 6, 3) #(input img channel, output channel, nxn kernel)\n","    self.conv2 = nn.Conv2d(6, 16, 3)\n","    \n","    self.fc1 = nn.Linear(16*6*6, 120)\n","    self.fc2 = nn.Linear(120, 84)\n","    self.fc3 = nn.Linear(84, 10)\n","  \n","  def forward(self, x):\n","    # Max pooling over a (2x2) window, squre라면 single number로도 가능\n","    x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n","    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","    x = x.view(-1, self.num_flat_features(x))\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.fc3(x)\n","    return x\n","  \n","  def num_flat_features(self, x):\n","    size = x.size()[1:] # all dimensions except the batch dimension\n","    num_features = 1 \n","    for s in size:\n","      num_features *= s \n","    return num_features\n","  \n","net = Net()\n","print(net)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0nEzzH9dcZUB","colab_type":"code","outputId":"37d4244f-94fc-47b9-b1b7-9906172ee32e","executionInfo":{"status":"ok","timestamp":1560842354195,"user_tz":-540,"elapsed":1362,"user":{"displayName":"Sangjoon Kwak","photoUrl":"","userId":"14338642773681962512"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# learnable parameters \n","\n","params = list(net.parameters())\n","print(len(params))\n","print(params[0].size())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10\n","torch.Size([6, 1, 3, 3])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7GhDyk4rdDhJ","colab_type":"code","outputId":"89de36f4-757f-42e9-a3d3-a6ba39ddb6c6","executionInfo":{"status":"ok","timestamp":1560842426076,"user_tz":-540,"elapsed":870,"user":{"displayName":"Sangjoon Kwak","photoUrl":"","userId":"14338642773681962512"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["input = torch.rand(1, 1, 32, 32) #(nSample, nChannels, Height, Width)\n","\n","out = net(input)\n","\n","print(out)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 0.1080, -0.0545,  0.0185,  0.0997, -0.0699, -0.0014,  0.0305,  0.1419,\n","         -0.0366, -0.0113]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VcGmhWDTfbq2","colab_type":"text"},"source":["무작위 gradient가 있는 backprop과 모든 매개 변수의 gradient buffer를 제로화"]},{"cell_type":"code","metadata":{"id":"NAg8sm_0dVMP","colab_type":"code","colab":{}},"source":["net.zero_grad()\n","out.backward(torch.rand(1,10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZV8YTVegDrv","colab_type":"text"},"source":["**torch.nn** 패키지는 미니 배치만을 지원한다. 전체 **torch.nn**가 단일 샘플이 아닌 미니 배치 샘플 임력만을 지원한다.\n","\n","예를 들면, **torch.nn.Conv2d**는 (nSample x nChannels x Height x Width)의 4D tensor를 이용한다. \n","\n","단일 샘플을 사용하는 경우, 가짜 batch dimension을 추가하기 위한 **input.unsqueeze(0)**을 사용한다. "]},{"cell_type":"markdown","metadata":{"id":"KFw7o_ibhRhP","colab_type":"text"},"source":["## Loss Function\n","\n","loss function은 (output, target) 형태의 입력값을 가진다. \n","output과 target 사이의 거리가 얼마나 먼가를 계산한다. \n","\n","nn package에는 많은 종류의 loss function이 있다. 가장 간단한 loss function 중 하나는 **nn.MSELoss**이며 mean squred error를 loss 값으로 사용한다. "]},{"cell_type":"code","metadata":{"id":"y7tLdsktf1IH","colab_type":"code","outputId":"7d4e3244-dc5f-4be5-9297-60d5595b6338","executionInfo":{"status":"ok","timestamp":1560843860429,"user_tz":-540,"elapsed":1013,"user":{"displayName":"Sangjoon Kwak","photoUrl":"","userId":"14338642773681962512"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["output = net(input)\n","target = torch.rand(10) # a dummy target, for example.\n","target = target.view(1, -1) # make it the same shape as output\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","\n","print(loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(0.3760, grad_fn=<MseLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"von_Lxz-izUq","colab_type":"code","outputId":"2e4ec142-9f42-45a1-83f3-6d374bf86f40","executionInfo":{"status":"ok","timestamp":1560844105877,"user_tz":-540,"elapsed":981,"user":{"displayName":"Sangjoon Kwak","photoUrl":"","userId":"14338642773681962512"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(loss.grad_fn) #MSELOSS\n","print(loss.grad_fn.next_functions[0][0]) # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) #ReLU"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<MseLossBackward object at 0x7f9b3b445e80>\n","<AddmmBackward object at 0x7f9b3b447be0>\n","<AccumulateGrad object at 0x7f9b3a3e6048>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XR2rliRpj8P-","colab_type":"text"},"source":["##Backprop\n","\n","에러를 역전파 하기 위해서 우리가 할 수 있는 것은 **loss.backward()**하는 것이다. \n","\n","존재하는 gradients들을 먼저 초기화 한다. 그렇지 않으면 gradients들이 이미 있던 gradients들과 섞이면서 쌓이게 된다. \n","\n","conv1의 bias gradients 들을 backward 전, 후 어떻게 바뀌는지 살펴보자. "]},{"cell_type":"code","metadata":{"id":"D2qsblOSi8Yg","colab_type":"code","colab":{}},"source":["net.zero_grad()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sEf4NwJkgsC","colab_type":"code","outputId":"f62a8fe2-1e96-4f95-bdc1-e3a01644cef7","executionInfo":{"status":"ok","timestamp":1560844370365,"user_tz":-540,"elapsed":1200,"user":{"displayName":"Sangjoon Kwak","photoUrl":"","userId":"14338642773681962512"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["print('conv1.bias.grad before backward')\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["conv1.bias.grad before backward\n","tensor([0., 0., 0., 0., 0., 0.])\n","conv1.bias.grad after backward\n","tensor([ 0.0007,  0.0057,  0.0017, -0.0069, -0.0030,  0.0052])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"27XnHxZzk3F0","colab_type":"text"},"source":["이것은 loss 가 neural network에서 어떻게 사용되는지를 보여준다."]},{"cell_type":"markdown","metadata":{"id":"gUPLTiUrk_e7","colab_type":"text"},"source":["## Update the weights\n","\n","**weight = weight - learning_rate x gradient**\n","\n","이 업데이트 식은 python에서 다음과 같이 계산할 수 있다."]},{"cell_type":"code","metadata":{"id":"W1qzBVGLkvyO","colab_type":"code","colab":{}},"source":["learning_rate = 0.01\n","\n","for f in net.parameters():\n","  f.data.sub_(f.grad.data*learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SI33HXM8lhTo","colab_type":"text"},"source":["하지만 신경망을 사용하면 SGD, Nesterov-SGD, Adam, RMSProp 등과 같은 다양한 업데이트 규칙을 사용할 수 있다. **torch.optim** 패키지에서 위 업데이트 방법들을 간단하게 사용할 수 있다. "]},{"cell_type":"code","metadata":{"id":"o24FHYpwlVR-","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","# optimizer 만들기\n","optimizer = optim.SGD(net.parameters(), lr = 0.01)\n","\n","optimizer.zero_grad() # initialize the gradient buffers\n","\n","output = net(input) # get output from the net\n","loss = criterion(output, target) # calculate the distance between output and target\n","loss.backward() # backpropagation\n","optimizer.step() # update the weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNieMlxjmX1E","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}